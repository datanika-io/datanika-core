"""rename pipeline to upload and create dbt pipelines table

Revision ID: e1f2a3b4c5d6
Revises: c8e4f2b1a9d3
Create Date: 2026-02-19 12:00:00.000000
"""

from collections.abc import Sequence

import sqlalchemy as sa
from alembic import op

revision: str = "e1f2a3b4c5d6"
down_revision: str | None = "c8e4f2b1a9d3"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    # 1. Rename old pipelines table (dlt extract+load) → uploads
    op.rename_table("pipelines", "uploads")
    op.execute("ALTER INDEX ix_pipelines_org_id RENAME TO ix_uploads_org_id")

    # 2. Update 'pipeline' → 'upload' in polymorphic type columns
    op.execute("UPDATE schedules SET target_type = 'upload' WHERE target_type = 'pipeline'")
    op.execute("UPDATE runs SET target_type = 'upload' WHERE target_type = 'pipeline'")
    op.execute(
        "UPDATE dependencies SET upstream_type = 'upload' WHERE upstream_type = 'pipeline'"
    )
    op.execute(
        "UPDATE dependencies SET downstream_type = 'upload' WHERE downstream_type = 'pipeline'"
    )
    op.execute(
        "UPDATE catalog_entries SET origin_type = 'upload' WHERE origin_type = 'pipeline'"
    )
    # Handle uppercase values from autogenerated migrations
    op.execute(
        "UPDATE catalog_entries SET origin_type = 'upload' WHERE origin_type = 'PIPELINE'"
    )

    # 3. Create new pipelines table (dbt orchestration)
    op.create_table(
        "pipelines",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("name", sa.String(255), nullable=False),
        sa.Column("description", sa.Text(), nullable=True),
        sa.Column("destination_connection_id", sa.BigInteger(), nullable=False),
        sa.Column(
            "command",
            sa.Enum(
                "build", "run", "test", "seed", "snapshot", "compile",
                name="dbtcommand", native_enum=False, length=20,
            ),
            nullable=False,
        ),
        sa.Column("full_refresh", sa.Boolean(), nullable=False, server_default="false"),
        sa.Column("models", sa.JSON(), nullable=True, server_default="[]"),
        sa.Column("custom_selector", sa.Text(), nullable=True),
        sa.Column(
            "status",
            sa.Enum(
                "draft", "active", "paused", "error",
                name="pipelinestatus_dbt", native_enum=False, length=20,
            ),
            nullable=False,
        ),
        sa.Column("org_id", sa.BigInteger(), nullable=False),
        sa.Column(
            "created_at", sa.DateTime(timezone=True),
            server_default=sa.text("now()"), nullable=False,
        ),
        sa.Column(
            "updated_at", sa.DateTime(timezone=True),
            server_default=sa.text("now()"), nullable=False,
        ),
        sa.Column("deleted_at", sa.DateTime(timezone=True), nullable=True),
        sa.PrimaryKeyConstraint("id"),
        sa.ForeignKeyConstraint(["org_id"], ["organizations.id"]),
        sa.ForeignKeyConstraint(["destination_connection_id"], ["connections.id"]),
    )
    op.create_index("ix_pipelines_org_id", "pipelines", ["org_id"])


def downgrade() -> None:
    # Drop new dbt pipelines table
    op.drop_index("ix_pipelines_org_id", table_name="pipelines")
    op.drop_table("pipelines")

    # Revert string values: 'upload' → 'pipeline'
    op.execute("UPDATE schedules SET target_type = 'pipeline' WHERE target_type = 'upload'")
    op.execute("UPDATE runs SET target_type = 'pipeline' WHERE target_type = 'upload'")
    op.execute(
        "UPDATE dependencies SET upstream_type = 'pipeline' WHERE upstream_type = 'upload'"
    )
    op.execute(
        "UPDATE dependencies SET downstream_type = 'pipeline' WHERE downstream_type = 'upload'"
    )
    op.execute(
        "UPDATE catalog_entries SET origin_type = 'PIPELINE' WHERE origin_type = 'upload'"
    )

    # Rename uploads back to pipelines
    op.execute("ALTER INDEX ix_uploads_org_id RENAME TO ix_pipelines_org_id")
    op.rename_table("uploads", "pipelines")
